{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle --quiet\n",
        "\n",
        "from google.colab import files\n",
        "print(\"Upload kaggle.json (from your Kaggle account page)\")\n",
        "uploaded = files.upload()  # choose kaggle.json\n",
        "\n",
        "import os\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "os.replace(\"kaggle.json\", \"/root/.kaggle/kaggle.json\")\n",
        "os.chmod(\"/root/.kaggle/kaggle.json\", 0o600)\n",
        "\n",
        "print(\"\\nTesting Kaggle API:\")\n",
        "!kaggle datasets list | head\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "IljE9D5T_5jW",
        "outputId": "a2f4191e-7ee7-4f27-840e-4fc831f86b6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload kaggle.json (from your Kaggle account page)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-da4c9203-f7b9-42b9-b9c7-1fd6197d2d95\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-da4c9203-f7b9-42b9-b9c7-1fd6197d2d95\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "\n",
            "Testing Kaggle API:\n",
            "ref                                                           title                                                     size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "------------------------------------------------------------  --------------------------------------------------  ----------  --------------------------  -------------  ---------  ---------------  \n",
            "wardabilal/spotify-global-music-dataset-20092025              Spotify Global Music Dataset (2009–2025)               1289021  2025-11-11 09:43:05.933000          12579        286  1.0              \n",
            "rohiteng/amazon-sales-dataset                                 Amazon Sales Dataset                                   4037578  2025-11-23 14:29:37.973000           4340         68  1.0              \n",
            "khushikyad001/ai-impact-on-jobs-2030                          AI Impact on Jobs 2030                                   87410  2025-11-09 17:58:05.410000           7176        160  1.0              \n",
            "sadiajavedd/students-academic-performance-dataset             Students_Academic_Performance_Dataset                     8907  2025-10-23 04:16:35.563000          16897        394  1.0              \n",
            "kundanbedmutha/exam-score-prediction-dataset                  Exam Score Prediction Dataset                           325454  2025-11-28 07:29:01.047000           2950         64  1.0              \n",
            "ayeshaseherr/buisness-sales                                   Buisness_Sales                                          954452  2025-11-24 08:51:12.203000           1883         40  1.0              \n",
            "emonsharkar/python-learning-and-exam-performance-dataset      Python Learning & Exam Performance Dataset               48915  2025-12-01 20:50:54.767000            660         27  1.0              \n",
            "sonalshinde123/social-media-mental-health-indicators-dataset  Social Media Mental Health Indicators Dataset            80135  2025-11-26 09:23:50.673000           2237         47  1.0              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "DATASET_SLUG = \"mohamed0422/amex-10k-sample\"  # your dataset\n",
        "RAW_DIR = \"amex_10k_raw\"\n",
        "os.makedirs(RAW_DIR, exist_ok=True)\n",
        "\n",
        "# Download the dataset\n",
        "!kaggle datasets download -d $DATASET_SLUG -p $RAW_DIR\n",
        "\n",
        "# Unzip the single zip file into RAW_DIR\n",
        "!unzip -o $RAW_DIR/*.zip -d $RAW_DIR\n",
        "\n",
        "print(\"\\nFiles in amex_10k_raw:\")\n",
        "!ls -R amex_10k_raw\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTfYgMvZ_7CW",
        "outputId": "ac281670-e999-43d1-e67a-6add24da2af4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/mohamed0422/amex-10k-sample\n",
            "License(s): unknown\n",
            "Downloading amex-10k-sample.zip to amex_10k_raw\n",
            " 79% 117M/148M [00:00<00:00, 1.19GB/s]\n",
            "100% 148M/148M [00:00<00:00, 905MB/s] \n",
            "Archive:  amex_10k_raw/amex-10k-sample.zip\n",
            "  inflating: amex_10k_raw/amex_10k/customer_ids_10k.txt  \n",
            "  inflating: amex_10k_raw/amex_10k/train_data_10k.csv  \n",
            "  inflating: amex_10k_raw/amex_10k/train_labels_10k.csv  \n",
            "\n",
            "Files in amex_10k_raw:\n",
            "amex_10k_raw:\n",
            "amex_10k  amex-10k-sample.zip\n",
            "\n",
            "amex_10k_raw/amex_10k:\n",
            "customer_ids_10k.txt  train_data_10k.csv  train_labels_10k.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, gc, json, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# ---- Correct paths based on your dataset structure ----\n",
        "# We saw from `ls -R` that files live in: amex_10k_raw/amex_10k/\n",
        "DATA_DIR = \"amex_10k_raw/amex_10k\"\n",
        "OUT_DIR = \"processed_amex_10k\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "train_data_path = os.path.join(DATA_DIR, \"train_data_10k.csv\")\n",
        "train_labels_path = os.path.join(DATA_DIR, \"train_labels_10k.csv\")\n",
        "\n",
        "print(\"Reading from:\")\n",
        "print(\"  \", train_data_path)\n",
        "print(\"  \", train_labels_path)\n",
        "\n",
        "# ------------------------------\n",
        "# 0) Load the 10k dataset\n",
        "# ------------------------------\n",
        "print(\"\\nLoading 10k data...\")\n",
        "df = pd.read_csv(train_data_path)\n",
        "labels = pd.read_csv(train_labels_path)\n",
        "print(\"train_data_10k:\", df.shape)\n",
        "print(\"train_labels_10k:\", labels.shape)\n",
        "\n",
        "# ------------------------------\n",
        "# 1) Basic cleaning & sorting\n",
        "# ------------------------------\n",
        "df[\"S_2\"] = pd.to_datetime(df[\"S_2\"], errors=\"coerce\")\n",
        "df = df.sort_values([\"customer_ID\", \"S_2\"]).reset_index(drop=True)\n",
        "\n",
        "# numeric feature columns (everything except id and date)\n",
        "num_cols = [c for c in df.columns if c not in [\"customer_ID\", \"S_2\"]]\n",
        "df[num_cols] = df[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "print(\"Number of numeric features:\", len(num_cols))\n",
        "\n",
        "# ------------------------------\n",
        "# 2) Aggregate per customer\n",
        "#    (last, mean, std, min, max, first)\n",
        "# ------------------------------\n",
        "aggs = {c: [\"last\", \"mean\", \"std\", \"min\", \"max\", \"first\"] for c in num_cols}\n",
        "agg_df = df.groupby(\"customer_ID\").agg(aggs)\n",
        "agg_df.columns = [f\"{c}_{s}\" for c, s in agg_df.columns]\n",
        "agg_df = agg_df.reset_index()\n",
        "\n",
        "# Add delta features: last - first\n",
        "for c in num_cols:\n",
        "    last_col = f\"{c}_last\"\n",
        "    first_col = f\"{c}_first\"\n",
        "    if last_col in agg_df.columns and first_col in agg_df.columns:\n",
        "        agg_df[f\"{c}_delta\"] = agg_df[last_col] - agg_df[first_col]\n",
        "\n",
        "print(\"Aggregated table shape:\", agg_df.shape)\n",
        "\n",
        "# ------------------------------\n",
        "# 3) Merge with labels\n",
        "# ------------------------------\n",
        "merged = agg_df.merge(labels, on=\"customer_ID\", how=\"inner\")\n",
        "print(\"Merged shape:\", merged.shape)\n",
        "\n",
        "X = merged.drop(columns=[\"customer_ID\", \"target\"])\n",
        "y = merged[\"target\"].astype(int)\n",
        "\n",
        "# ------------------------------\n",
        "# 4) Train / validation split\n",
        "# ------------------------------\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "print(\"X_train:\", X_train.shape)\n",
        "print(\"X_valid:\", X_valid.shape)\n",
        "\n",
        "# ------------------------------\n",
        "# 5) Impute missing values (robust)\n",
        "# ------------------------------\n",
        "num_imputer = SimpleImputer(strategy=\"median\")\n",
        "\n",
        "# Fit on train, transform both\n",
        "X_train_np = num_imputer.fit_transform(X_train)\n",
        "X_valid_np = num_imputer.transform(X_valid)\n",
        "\n",
        "# Number of features actually returned by the imputer\n",
        "n_features = X_train_np.shape[1]\n",
        "\n",
        "# Use only the first n_features column names to match the array width\n",
        "cols = list(X_train.columns[:n_features])\n",
        "\n",
        "# Rebuild DataFrames with matching shapes\n",
        "X_train = pd.DataFrame(X_train_np, columns=cols, index=X_train.index)\n",
        "X_valid = pd.DataFrame(X_valid_np, columns=cols, index=X_valid.index)\n",
        "\n",
        "# ------------------------------\n",
        "# 6) Save processed outputs\n",
        "# ------------------------------\n",
        "X_train.to_parquet(os.path.join(OUT_DIR, \"X_train.parquet\"))\n",
        "X_valid.to_parquet(os.path.join(OUT_DIR, \"X_valid.parquet\"))\n",
        "y_train.to_csv(os.path.join(OUT_DIR, \"y_train.csv\"), index=False)\n",
        "y_valid.to_csv(os.path.join(OUT_DIR, \"y_valid.csv\"), index=False)\n",
        "\n",
        "with open(os.path.join(OUT_DIR, \"features.json\"), \"w\") as f:\n",
        "    json.dump(\n",
        "        {\n",
        "            \"numeric_raw\": num_cols,   # numeric columns before aggregation\n",
        "            \"model_features\": cols,    # actual features used by the model\n",
        "            \"categorical\": []\n",
        "        },\n",
        "        f,\n",
        "        indent=2\n",
        "    )\n",
        "\n",
        "print(\"\\n✅ Done. Files in processed_amex_10k:\")\n",
        "!ls -lh processed_amex_10k\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q78G2-fxCGeB",
        "outputId": "c0f937c2-5f42-4e79-9ea1-821ed143c554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading from:\n",
            "   amex_10k_raw/amex_10k/train_data_10k.csv\n",
            "   amex_10k_raw/amex_10k/train_labels_10k.csv\n",
            "\n",
            "Loading 10k data...\n",
            "train_data_10k: (120644, 190)\n",
            "train_labels_10k: (10000, 2)\n",
            "Number of numeric features: 188\n",
            "Aggregated table shape: (10000, 1317)\n",
            "Merged shape: (10000, 1318)\n",
            "X_train: (8000, 1316)\n",
            "X_valid: (2000, 1316)\n",
            "\n",
            "✅ Done. Files in processed_amex_10k:\n",
            "total 99M\n",
            "-rw-r--r-- 1 root root  24K Dec 10 20:09 features.json\n",
            "-rw-r--r-- 1 root root  79M Dec 10 20:09 X_train.parquet\n",
            "-rw-r--r-- 1 root root  20M Dec 10 20:09 X_valid.parquet\n",
            "-rw-r--r-- 1 root root  16K Dec 10 20:09 y_train.csv\n",
            "-rw-r--r-- 1 root root 4.0K Dec 10 20:09 y_valid.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Member 1: Data Collection, Cleaning, and Preprocessing (Mohamed Mohamed)**\n",
        "\n",
        "As Member 1, my role in the project was to perform **data collection, cleaning, and preprocessing** for the American Express Default Prediction dataset. Because the original AMEX dataset is extremely large (≈ 40GB uncompressed), the preprocessing pipeline needed to be efficient, scalable, and compatible with Google Colab and Kaggle’s computational environment.\n",
        "\n",
        "Below is a detailed description of every step performed.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Dataset Collection via Kaggle Notebook**\n",
        "\n",
        "The AMEX dataset cannot be downloaded directly to Colab due to its size.\n",
        "Instead, I used a **Kaggle Notebook** to access the full competition data from:\n",
        "\n",
        "```\n",
        "/kaggle/input/amex-default-prediction\n",
        "```\n",
        "\n",
        "### **Steps:**\n",
        "\n",
        "1. Loaded the full `train_labels.csv` (≈458k customers).\n",
        "2. Selected the **first 10,000 unique customers** to create a development subset.\n",
        "3. Saved:\n",
        "\n",
        "   * `train_labels_10k.csv`\n",
        "   * `customer_ids_10k.txt`\n",
        "\n",
        "### **Chunk Streaming of train_data.csv**\n",
        "\n",
        "Because the full `train_data.csv` contains **over 55 million rows**, I streamed it in 1,000,000-row chunks:\n",
        "\n",
        "* Each chunk was filtered to keep only rows belonging to the 10k selected customers.\n",
        "* This approach avoids memory overflow.\n",
        "\n",
        "### **Output Files Produced:**\n",
        "\n",
        "```\n",
        "train_data_10k.csv      (~120,644 rows)\n",
        "train_labels_10k.csv    (10,000 rows)\n",
        "customer_ids_10k.txt\n",
        "```\n",
        "\n",
        "These files were then packaged and uploaded as a **private Kaggle dataset**:\n",
        "**`mohamed0422/amex-10k-sample`**\n",
        "\n",
        "This allowed Colab to download a small dataset (~356MB) instead of the full 40+GB version.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Loading the Subset into Google Colab**\n",
        "\n",
        "Inside Colab:\n",
        "\n",
        "* Configured the Kaggle API using `kaggle.json`\n",
        "* Downloaded my custom dataset using:\n",
        "\n",
        "```\n",
        "kaggle datasets download -d mohamed0422/amex-10k-sample\n",
        "```\n",
        "\n",
        "* Unzipped the dataset into:\n",
        "\n",
        "```\n",
        "amex_10k_raw/amex_10k/\n",
        "```\n",
        "\n",
        "Files now available:\n",
        "\n",
        "```\n",
        "train_data_10k.csv\n",
        "train_labels_10k.csv\n",
        "customer_ids_10k.txt\n",
        "```\n",
        "\n",
        "This dataset was used for all preprocessing steps.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Cleaning the Data**\n",
        "\n",
        "### ✅ **Converted S_2 to datetime**\n",
        "\n",
        "```python\n",
        "df[\"S_2\"] = pd.to_datetime(df[\"S_2\"], errors=\"coerce\")\n",
        "```\n",
        "\n",
        "### ✅ **Sorted by customer_ID and date**\n",
        "\n",
        "AMEX data is sequential; sorting ensures integrity of “first” and “last” metrics.\n",
        "\n",
        "### ✅ **Converted all numeric features**\n",
        "\n",
        "```python\n",
        "df[num_cols] = df[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
        "```\n",
        "\n",
        "This standardizes the dataset and prepares it for aggregation and modeling.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Customer-Level Feature Engineering (Aggregation)**\n",
        "\n",
        "AMEX data includes multiple statements per customer.\n",
        "For each numeric column (188 total), I computed:\n",
        "\n",
        "| Metric                   | Meaning                   |\n",
        "| ------------------------ | ------------------------- |\n",
        "| **last**                 | Most recent value         |\n",
        "| **first**                | Earliest value            |\n",
        "| **mean**                 | Average across statements |\n",
        "| **std**                  | Variability               |\n",
        "| **min**                  | Minimum value             |\n",
        "| **max**                  | Maximum value             |\n",
        "| **delta = last − first** | Direction/change          |\n",
        "\n",
        "### Result:\n",
        "\n",
        "```\n",
        "Aggregated feature shape: (10,000 customers, 1,317 features)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Merging With Target Labels**\n",
        "\n",
        "After aggregation, I merged the 10k aggregated features with the sampled labels:\n",
        "\n",
        "```python\n",
        "merged = agg_df.merge(labels, on=\"customer_ID\")\n",
        "```\n",
        "\n",
        "Final merged shape:\n",
        "\n",
        "```\n",
        "10,000 customers × 1,318 columns (features + target)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Train/Validation Split**\n",
        "\n",
        "I applied an **80/20 stratified split**:\n",
        "\n",
        "```\n",
        "X_train: (8000, 1316)\n",
        "X_valid: (2000, 1316)\n",
        "```\n",
        "\n",
        "Stratification maintains the correct ratio of defaults vs. non-defaults.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Handling Missing Values**\n",
        "\n",
        "Using `SimpleImputer(strategy=\"median\")`, I imputed all missing numeric values.\n",
        "\n",
        "A robust fix was added because imputation slightly reduced the number of columns.\n",
        "The final solution rebuilt the DataFrame using only the imputed feature columns.\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Saving Final Preprocessed Outputs**\n",
        "\n",
        "All final cleaned and structured datasets were saved to:\n",
        "\n",
        "```\n",
        "processed_amex_10k/\n",
        "```\n",
        "\n",
        "Files included:\n",
        "\n",
        "| File              | Description                                   |\n",
        "| ----------------- | --------------------------------------------- |\n",
        "| `X_train.parquet` | Model-ready training features                 |\n",
        "| `X_valid.parquet` | Model-ready validation features               |\n",
        "| `y_train.csv`     | Training labels                               |\n",
        "| `y_valid.csv`     | Validation labels                             |\n",
        "| `features.json`   | Metadata for numeric and final model features |\n",
        "\n",
        "---\n",
        "\n",
        "## **9. Completion Summary**\n",
        "\n",
        "All Member 1 responsibilities have been fully completed:\n",
        "\n",
        "### ✔ Data collection (via Kaggle + subset creation)\n",
        "\n",
        "### ✔ Data cleaning (datetime parsing, numeric coercion)\n",
        "\n",
        "### ✔ Feature engineering (aggregation + deltas)\n",
        "\n",
        "### ✔ Missing value imputation\n",
        "\n",
        "### ✔ Train/validation split\n",
        "\n",
        "### ✔ Exporting final processed files\n",
        "\n",
        "### ✔ Documented pipeline for team use\n",
        "\n",
        "These outputs allow the rest of the team to:\n",
        "\n",
        "* Perform EDA\n",
        "* Train baseline and advanced models\n",
        "* Compare performance across methods\n",
        "* Build the final AMEX default prediction pipeline\n",
        "\n",
        "---\n",
        "\n",
        "## **Final Deliverables for Member 1**\n",
        "\n",
        "```\n",
        "train_data_10k.csv\n",
        "train_labels_10k.csv\n",
        "customer_ids_10k.txt\n",
        "processed_amex_10k/X_train.parquet\n",
        "processed_amex_10k/X_valid.parquet\n",
        "processed_amex_10k/y_train.csv\n",
        "processed_amex_10k/y_valid.csv\n",
        "processed_amex_10k/features.json\n",
        "```\n"
      ],
      "metadata": {
        "id": "YzxKEK3vDJar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Member 3: Model Implementation: Logistic Regression, Random Forest (Nathan Morales)"
      ],
      "metadata": {
        "id": "nj4mPKgTC6Q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from joblib import dump\n",
        "\n",
        "#Reading the dataset given from member 1\n",
        "X_train = pd.read_parquet(\"processed_amex_10k/X_train.parquet\")\n",
        "X_valid = pd.read_parquet(\"processed_amex_10k/X_valid.parquet\")\n",
        "y_train = pd.read_csv(\"processed_amex_10k/y_train.csv\")[\"target\"]\n",
        "y_valid = pd.read_csv(\"processed_amex_10k/y_valid.csv\")[\"target\"]\n",
        "\n",
        "#Splitting the dataset\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(\n",
        "    X_valid, y_valid, test_size=0.5, stratify=y_valid, random_state=42\n",
        ")\n",
        "\n",
        "#Performing Logistic Regression and hyperparameter tuning\n",
        "log_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    (\"model\", LogisticRegression())])\n",
        "\n",
        "log_param_grid = {\n",
        "    'model__C': [0.1, 1],\n",
        "    'model__penalty': ['l2'],\n",
        "    'model__solver': ['liblinear']\n",
        "}\n",
        "\n",
        "log_grid_search = RandomizedSearchCV(log_pipeline, log_param_grid, cv=3, scoring='roc_auc', n_iter=50, random_state=42)\n",
        "log_grid_search.fit(X_train, y_train)\n",
        "\n",
        "#Performing Random Forest classifier and hyperparameter tuning\n",
        "raf_pipeline = Pipeline([\n",
        "    (\"model\", RandomForestClassifier())])\n",
        "\n",
        "raf_param_grid = {\n",
        "    'model__n_estimators': [100],\n",
        "    'model__max_depth': [None, 10],\n",
        "    'model__min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "raf_grid_search = RandomizedSearchCV(raf_pipeline, raf_param_grid, cv=3, scoring='roc_auc', n_iter=50, random_state=42)\n",
        "raf_grid_search.fit(X_train, y_train)\n",
        "\n",
        "#Evaluation\n",
        "log_best_model = log_grid_search.best_estimator_\n",
        "raf_best_model = raf_grid_search.best_estimator_\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "  y_pred = model.predict(X_test)\n",
        "  y_pred_proba = model.predict_proba(X_test)[:,1]\n",
        "  return {\n",
        "      \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "      \"Precision\": precision_score(y_test, y_pred),\n",
        "      \"Recall\": recall_score(y_test, y_pred),\n",
        "      \"F1\": f1_score(y_test, y_pred),\n",
        "      \"ROC AUC\": roc_auc_score(y_test, y_pred_proba),\n",
        "      \"Confusion Matrix\": confusion_matrix(y_test, y_pred)\n",
        "  }\n",
        "\n",
        "\n",
        "comparison_table = pd.DataFrame({\n",
        "    \"Model\": [\"Logistic Regression\", \"Random Forest Classifier\"],\n",
        "    \"Accuracy\": [log_eval[\"Accuracy\"], raf_eval[\"Accuracy\"]],\n",
        "    \"Precision\": [log_eval[\"Precision\"], raf_eval[\"Precision\"]],\n",
        "    \"Recall\": [log_eval[\"Recall\"], raf_eval[\"Recall\"]],\n",
        "    \"F1\": [log_eval[\"F1\"], raf_eval[\"F1\"]],\n",
        "    'ROC AUC': [log_eval[\"ROC AUC\"], raf_eval[\"ROC AUC\"]]\n",
        "\n",
        "})\n",
        "\n",
        "lr_preds = log_best_model.predict(X_test)\n",
        "rf_preds = raf_best_model.predict(X_test)\n",
        "\n",
        "print(\"Logistic Regression Confusion Matrix:\\n\", confusion_matrix(y_valid, lr_preds))\n",
        "print(\"Random Forest Confusion Matrix:\\n\", confusion_matrix(y_valid, rf_preds))\n",
        "\n",
        "print(\"\\nComparison Table:\")\n",
        "print(comparison_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6djrzokHwCXW",
        "outputId": "06ff3ba3-a870-474f-f1c7-a6ca9a7a8178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Confusion Matrix:\n",
            " [[540 195]\n",
            " [202  63]]\n",
            "Random Forest Confusion Matrix:\n",
            " [[537 198]\n",
            " [199  66]]\n",
            "\n",
            "Comparison Table:\n",
            "                      Model  Accuracy  Precision    Recall        F1  \\\n",
            "0       Logistic Regression     0.879   0.779070  0.758491  0.768642   \n",
            "1  Random Forest Classifier     0.893   0.811024  0.777358  0.793834   \n",
            "\n",
            "    ROC AUC         Confusion Matrix  \n",
            "0  0.941050  [[678  57]\\n [ 64 201]]  \n",
            "1  0.950394  [[687  48]\\n [ 59 206]]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "McZhc4jODJaC"
      }
    }
  ]
}